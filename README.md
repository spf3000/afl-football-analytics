# AFL Football Analytics - Data Pipeline

A modern data engineering project for AFL match analysis, built with dbt, Databricks, and functional programming principles.

## ğŸ¯ Project Goals

- **Learning**: Master Databricks and dbt while building a real data pipeline
- **Certification**: Pass Databricks Data Engineer Associate certification (3-4 months)
- **Long-term**: Become a Databricks specialist with deep technical knowledge
- **Analytics**: Build advanced AFL analytics (Elo ratings, predictions, Brownlow predictor)
- **FP Principles**: Apply functional programming patterns (idempotency, composition, purity)

## ğŸ“ Project Structure

```
afl-football-analytics/
â”œâ”€â”€ afl_analytics/              # dbt project (data pipeline)
â”‚   â”œâ”€â”€ dbt_project.yml         # Main dbt config
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/             # Raw â†’ typed tables
â”‚   â”‚   â”œâ”€â”€ silver/             # Cleaned & core entities (in development)
â”‚   â”‚   â””â”€â”€ gold/               # Analytics & aggregations (future)
â”‚   â”œâ”€â”€ scripts/                 # Run on databricks
â”‚   â””â”€â”€ data/                   # data - not committed
    â”” seeds/                    #


â”‚
â”œâ”€â”€ scripts/                    # Python preprocessing scripts
â”‚   â””â”€â”€ stg_match_results.py  # Parse .txt files â†’ raw table
â”‚
â”œâ”€â”€ notebooks/                  # Analysis notebooks
â”‚   â”œâ”€â”€ crowd_analysis.ipynb    # (your existing work)
â”‚   â””â”€â”€ match_winner.ipynb
â”‚
â”œâ”€â”€ data/                       # Local data (for reference)
â”‚   â””â”€â”€ (your CSV files)
â”‚
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ COMMANDS_REFERENCE.md       # Quick command reference
```

## ğŸ—ï¸ Architecture

### Medallion Architecture (Bronze â†’ Silver â†’ Gold)

```
Raw .txt Files (Volume)
    â†“
Python Parser (scripts/stg_match_results.py)
    â†“
raw.match_results_parsed (Raw table)
    â†“
dbt Bronze Model (models/bronze/stg_match_results.sql)
    â†“
bronze.stg_match_results (Typed, tested, documented)
    â†“
[Future: Silver models - dim_teams, dim_venues, fct_matches]
    â†“
[Future: Gold models - Elo, predictions, aggregations]
```

### Functional Programming Principles

**Idempotency:**

- Python parser: Same .txt files â†’ same parsed table (can rerun safely)
- dbt models: Same input â†’ same output (can rerun safely)
- No side effects, no hidden state

**Composition:**

- Bronze models compose into Silver models
- Silver models compose into Gold models
- Small, single-purpose transformations

**Purity:**

- Each dbt model is a pure function: `output = f(inputs)`
- Deterministic transformations
- Easy to test and reason about

**Immutability:**

- Bronze layer stays raw
- Transformations create new tables, never UPDATE in place
- Version history preserved

## ğŸš€ Current Pipeline Status

### âœ… Completed

**Bronze Layer:**

- [x] Raw .txt files uploaded to Databricks Volume
- [x] Python parser handles messy text format
- [x] `raw.match_results_parsed` table created (2020-2024)
- [x] dbt Bronze model creates `bronze.stg_match_results`
- [x] Data quality tests passing
- [x] Documentation generated

**Data Available:**

- Match results (2020-2024): ~1000+ matches
- Teams, venues, scores, crowds (NULL for 2020 COVID year)
- Calculated fields: margin, result

### ğŸ”¨ In Progress / Next Steps

**Silver Layer (Next):**

- [ ] `dim_teams` - Team dimension table
- [ ] `dim_venues` - Venue dimension table
- [ ] `fct_matches` - Fact table (matches with all joins)
- [ ] Standardize team/venue names
- [ ] Add team metadata (colors, nicknames)

**Gold Layer (Future):**

- [ ] Elo rating system
- [ ] Match prediction model
- [ ] Brownlow predictor
- [ ] Crowd analysis aggregations
- [ ] Game style clustering ("game on our terms")
- [ ] Turnover analysis by game time

**Other:**

- [ ] Migrate existing notebooks to use Bronze tables
- [ ] Add crowd data as separate source

## ğŸ”§ Tech Stack

- **Data Platform**: Databricks (Unity Catalog, Volumes, SQL Warehouses)
- **Transformation**: dbt (SQL-based transformations)
- **Language**: Python (preprocessing), SQL (transformations)
- **Package Management**: uv (Python), dbt (SQL packages)
- **Version Control**: Git + GitHub
- **Storage Format**: Delta Lake (ACID transactions, time travel)
- **Testing**: dbt tests (data quality), dbt_utils package

## ğŸ“Š Data Sources

### Match Results (.txt files)

- **Source**: Copy/paste from AFL websites
- **Format**: Tab-delimited text files (messy!)
- **Years**: 2020-2024 (one file per year)
- **Location**: `/Volumes/afl_analytics_dev/raw/afl_raw_files/`
- **Parser**: `scripts/stg_match_results.py`
- **Challenges**:
  - Empty crowd column for 2020 (COVID)
  - Multiple lines per match
  - Inconsistent spacing
  - BYE rows to filter

### Future Data Sources

- Crowd analysis data
- Player statistics (if available)

## ğŸƒ Quick Start

### Prerequisites

- Databricks workspace access
- Python 3.11+
- uv package manager
- Git

### Initial Setup

1. **Clone Repository:**

   ```bash
   git clone <your-repo-url>
   cd afl-football-analytics
   ```

2. **Install Dependencies:**

   ```bash
   uv sync
   ```

3. **Configure Databricks Connection:**

   ```bash
   cp afl_analytics/profiles.yml.example ~/.dbt/profiles.yml
   # Edit ~/.dbt/profiles.yml with your Databricks credentials
   export DATABRICKS_TOKEN='your-token-here'
   ```

4. **Create Databricks Resources:**

   ```sql
   -- In Databricks SQL Editor
   CREATE CATALOG IF NOT EXISTS afl_analytics_dev;
   CREATE SCHEMA IF NOT EXISTS afl_analytics_dev.raw;
   CREATE SCHEMA IF NOT EXISTS afl_analytics_dev.bronze;
   CREATE SCHEMA IF NOT EXISTS afl_analytics_dev.silver;
   CREATE SCHEMA IF NOT EXISTS afl_analytics_dev.gold;
   CREATE VOLUME IF NOT EXISTS afl_analytics_dev.raw.afl_raw_files;
   ```

5. **Upload Raw Data:**
   - Navigate to Catalog â†’ `afl_analytics_dev` â†’ `raw` â†’ `Volumes` â†’ `afl_raw_files`
   - Upload your .txt files: `real_afl_attendance_YYYY.txt`

6. **Run Parser Script:**

   ```bash
   # In Databricks notebook or Repos
   # Open: scripts/stg_match_results.py
   # Click "Run All"
   ```

7. **Run dbt:**

   ```bash
   cd afl_analytics
   uv run dbt deps    # Install packages (first time only)
   uv run dbt run     # Create Bronze tables
   uv run dbt test    # Run data quality tests
   ```

### Daily Workflow

```bash
# When you get new data (e.g., 2025 season) or schema changes

# 1. Upload new .txt file to Volume (via Databricks UI) if applicable

# 2. Update and Run parser script in Databricks
# Edit scripts/stg_match_results.py if adding new years.
# IMPORTANT: Run this in Databricks to update raw.match_results_parsed
# especially if you've modified the parsing logic (e.g., adding match_datetime).

# 3. Run dbt locally
cd afl_analytics
uv run dbt seed                 # Update seeds (teams, venues)
uv run dbt run                  # Run all models
uv run dbt run --select bronze  # Run bronze layer only
uv run dbt run --select silver  # Run silver layer only
uv run dbt test                 # Verify data quality

# 4. Use in notebooks!
```

## ğŸ“š Documentation

### Generate dbt Docs

```bash
cd afl_analytics
uv run dbt docs generate
uv run dbt docs serve  # Opens browser at http://localhost:8080
```

**What you'll see:**

- Data lineage diagram (visual flow of transformations)
- Column-level documentation
- Test results
- SQL code for each model
- Source â†’ Model â†’ Usage relationships

### Key Documentation Files

- `README.md` (this file) - Project overview
- `COMMANDS_REFERENCE.md` - Quick command reference
- `afl_analytics/models/bronze/_models.yml` - Column documentation
- `docs/football-databricks-learning-roadmap.md` - Learning timeline
- `docs/football-dbt-project-structure.md` - Detailed architecture

## ğŸ§ª Testing

### Run All Tests

```bash
cd afl_analytics
uv run dbt test
```

### Run Tests for Specific Model

```bash
uv run dbt test --select stg_match_results
```

### What Tests Check

- **not_null**: Critical columns have no NULLs
- **accepted_range**: Scores between 0-300
- **accepted_values**: Result is home_win/away_win/draw
- Custom tests in `tests/` directory

## ğŸ” Exploration & Analysis

### Using Bronze Data in Notebooks

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# Read from Bronze
matches = spark.table("afl_analytics_dev.bronze.stg_match_results")

# Analyze
matches.groupBy("season").count().show()
matches.filter("season == 2024").show()

# Your analysis here!
```

### Query in Databricks SQL

```sql
-- Average scores by season
SELECT 
    season,
    AVG(home_score) as avg_home_score,
    AVG(away_score) as avg_away_score,
    AVG(margin) as avg_margin
FROM afl_analytics_dev.bronze.stg_match_results
GROUP BY season
ORDER BY season;

-- Biggest wins by margin
SELECT season, round, home_team, away_team, margin
FROM afl_analytics_dev.bronze.stg_match_results
ORDER BY ABS(margin) DESC
LIMIT 10;
```

## ğŸ“– Learning Resources

### Databricks Certification Prep

- Databricks Academy (free courses)
- Practice exam (before attempting)
- Focus areas: Delta Lake, Spark SQL, performance tuning
- Timeline: 3-4 months to Associate certification

### FP + Data Engineering

- Medallion architecture = function composition
- dbt models = pure functions
- Delta Lake = immutable event log
- Idempotent transformations = safe reruns

### Reference Documentation

- `/docs/` folder contains detailed guides
- dbt docs: `dbt docs serve`
- Databricks docs: <https://docs.databricks.com>

## ğŸ› Troubleshooting

### Common Issues

**"dbt command not found"**

```bash
# Use uv run prefix
uv run dbt --version

# Or activate environment first
uv sync
source .venv/bin/activate
dbt --version
```

**"Source 'match_results_parsed' not found"**

```bash
# Make sure parser script ran successfully
# Check in Databricks SQL:
SHOW TABLES IN afl_analytics_dev.raw;
SELECT COUNT(*) FROM afl_analytics_dev.raw.match_results_parsed;
```

**"Column not found" or "UNRESOLVED_COLUMN" errors in dbt**

- This usually means the Python parser hasn't been run on Databricks since a schema change (e.g., adding `match_datetime`).
- Run the `scripts/stg_match_results.py` script in Databricks to refresh the raw table.
- Check `_sources.yml` matches actual table schema: `DESCRIBE afl_analytics_dev.raw.match_results_parsed`

**Tests failing**

```bash
# See which tests failed
uv run dbt test --select stg_match_results

# Check the data
SELECT * FROM afl_analytics_dev.bronze.stg_match_results
WHERE home_score IS NULL;  -- Find the problem rows
```

## ğŸ¯ Project Milestones

### Completed âœ…

- [x] Project structure set up
- [x] Git repository initialized
- [x] Databricks connection configured
- [x] Raw data uploaded to Volume
- [x] Python parser working
- [x] Bronze layer complete
- [x] dbt tests passing
- [x] Documentation generated
- [x] Match datetime parsing implemented

### Next 2 Weeks ğŸ”¨

- [ ] Build Silver layer (dim_teams complete, dim_venues, fct_matches next)
- [ ] Migrate one existing notebook to use Bronze tables
- [ ] Start Elo rating calculation in Gold layer

### Next Month ğŸ“…

- [ ] Complete Gold layer (Elo, predictions, aggregations)
- [ ] All notebooks migrated to use dbt tables
- [ ] Start Databricks certification study

### 3-4 Months ğŸ“

- [ ] Pass Databricks Data Engineer Associate certification
- [ ] Advanced analytics complete (Brownlow, game clustering)
- [ ] Blog posts on FP + data engineering

## ğŸ¤ Contributing / Development

### Making Changes

1. **Edit locally in Git repo**
2. **Commit changes**

   ```bash
   git add .
   git commit -m "Add feature X"
   git push
   ```

3. **Sync in Databricks Repos**
   - Go to Repos â†’ Your repo
   - Click Git â†’ Pull
4. **Test changes**

   ```bash
   uv run dbt run
   uv run dbt test
   ```

### Code Style

- SQL: Lowercase keywords, clear indentation
- Python: Follow PEP 8, use type hints
- dbt: One model per file, clear naming
- Tests: Add tests for all new models
- Documentation: Update _models.yml for new columns

## ğŸ“ Contact / Questions

- **Project Owner**: Alan (that's you!)
- **Repository**: <https://github.com:spf3000/afl-football-analytics.git>
- **Databricks Workspace**: <https://dbc-1a2d7fa9-029a.cloud.databricks.com/browse/folders/2908617711815098?o=157478337743706>

## ğŸ“ Notes

- 2020 season has NULL crowds (COVID)
- Parser script handles messy .txt format
- dbt runs can be scheduled in Databricks Jobs (future)
- Delta Lake provides time travel: `SELECT * FROM table VERSION AS OF 1`
- All transformations are idempotent - safe to rerun anytime

---

**Last Updated**: January 2026
**Status**: Bronze layer complete, Silver layer (dim_teams) in progress
**Learning Focus**: Databricks certification prep + FP principles
